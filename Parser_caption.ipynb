{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentiment analysis with textblob at the movie's caption. use time interval 1 sec, find positive, negative, neutral sentiments for each video. Have a dictionary with: the title of caption, the intervals  , the sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/mscuser/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pysrt\n",
    "from datetime import date, datetime, timedelta, time\n",
    "import pysrt\n",
    "from textblob import TextBlob\n",
    "import matplotlib\n",
    "from matplotlib import style\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (16.0, 9.0)\n",
    "style.use('fivethirtyeight')\n",
    "import os\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import os, re, sys\n",
    "from stat import *\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import csv\n",
    "\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('sentiwordnet')\n",
    "import re\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.downloader.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove segments at which only tags are writtesn as caption text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(\"\\[(.*?)\\]|\\-\\[(.*?)\\]\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if (bool(pattern.match('[ Laughter, cheering,and applause ]'))):\n",
    "    print \"nai\"\n",
    "else:\n",
    "    print \"no\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling srt Subtitles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Loading the Subtitle\n",
    "subs = pysrt.open('/home/mscuser/multimodal/multimodal_audio/Travis Scott Shows Off His Broadway Musical Abilities-QV9nArq7phA.en.srt')\n",
    "subtitles={}\n",
    "len_subs=len(subs)\n",
    "subs_len=0\n",
    "for i in range(len_subs):\n",
    "    if (bool(pattern.match(subs[i].text_without_tags))):\n",
    "        print \"--------- exclude tag --------\"\n",
    "    else:\n",
    "        sub = subs[i]\n",
    "        print  \"**\"+str(subs_len)+\"***[\"+ str(sub.start)+\"]\"+\"[\"+str(sub.end)+\"]\" +sub.text_without_tags\n",
    "\n",
    "        # Subtitle text\n",
    "        text = sub.text\n",
    "        text_without_tags = sub.text_without_tags\n",
    "\n",
    "        # Start and End time\n",
    "        start = sub.start.to_time()\n",
    "        end = sub.end.to_time()\n",
    "        subtitles[subs_len]=[[text,text_without_tags,start,end]]\n",
    "        subs_len=subs_len+1\n",
    "\n",
    "  #  print sub.text\n",
    "  #  print sub.start\n",
    "  #  print sub.end\n",
    "   # print sub.end-sub.start\n",
    "print len(subtitles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# textblob sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textblob_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    return polarity\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''VADER produces four sentiment metrics from these word ratings, \n",
    "which you can see below. The first three, positive, neutral and negative, \n",
    "represent the proportion of the text that falls into those categories. \n",
    "As you can see, our example sentence was rated as 45% positive, \n",
    "55% neutral and 0% negative. The final metric, the compound score, \n",
    "is the sum of all of the lexicon ratings (1.9 and 1.8 in this case) \n",
    "which have been standardised to range between -1 and 1. \n",
    "In this case, our example sentence has a rating of 0.69, which is pretty strongly positive.'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "def vader_sentiment(text):\n",
    "    polarity=sid.polarity_scores(text)['compound']\n",
    "    return polarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiwordnet(text):\n",
    "    polarity= swn.senti_synsets(text)\n",
    "    print polarity\n",
    "    polarity.pos_score()\n",
    "    polarity.neg_score()\n",
    "    polarity.obj_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_sentiment(file):\n",
    "    # Reading Subtitleget_sentiment\n",
    "    subs = pysrt.open(file, encoding='iso-8859-1')\n",
    "    n = len(subs)\n",
    "    print n\n",
    "    # List to store the time periods\n",
    "    intervals = []\n",
    "    sentiments_text_blob = []\n",
    "    sentiments_vader = []\n",
    "    subs_len=-1\n",
    "\n",
    "    \n",
    "    \n",
    "    # Collect and combine all the text in each time interval\n",
    "    for j in range(n):\n",
    "        text = \"\"\n",
    "        if (bool(pattern.match(subs[j].text_without_tags))):\n",
    "            print \"--------- exclude tag --------\" + str(j)\n",
    "            \n",
    "        else:\n",
    "            # Finding all subtitle text in the each time interval\n",
    "            segment=subs[j].end-subs[j].start\n",
    "    \n",
    "            #print \"end:\" + str(subs[j].end)+ \"strat:\" +str(subs[j].start) + \"segment:\"+str(segment) + \"segment.to_time:\"+str(segment.to_time())\n",
    "            intervals.append(subs[j].start + segment)\n",
    "            subs_len=subs_len+1\n",
    "            #for example [0,4],[4,6]->4,6\n",
    "            if subs[j].end.to_time() <= (subs[j].start+ intervals[subs_len]).to_time():\n",
    "                 text += subs[j].text_without_tags + \" \"\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "            # Sentiment Analysis with TextBlob\n",
    "\n",
    "            sentiment_blob=textblob_sentiment(text)\n",
    "            sentiments_text_blob.append(sentiment_blob)\n",
    "\n",
    "            # Sentiment Analysis with Vader\n",
    "            sentiment_vader=vader_sentiment(text)\n",
    "            sentiments_vader.append(sentiment_vader)\n",
    "        \n",
    "\n",
    "    #find the avrage of the 2 different sentiment analysis\n",
    "    avg_sentiments=[(a_i + b_i)/float(2) for a_i, b_i in zip(sentiments_vader, sentiments_text_blob)]\n",
    "    print len(avg_sentiments)\n",
    "    print len(intervals)\n",
    "    return (intervals, sentiments_text_blob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility to find average sentiment\n",
    "def average(y):\n",
    "    avg = float(sum(y))/len(y)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dict={id, path, , intervals, sentiments}\n",
    "dict={}\n",
    "\n",
    "i=interval_segments=sentiment_segments=0\n",
    "def walktree(TopMostPath, callback):\n",
    "\n",
    "    '''recursively descend the directory tree rooted at TopMostPath,\n",
    "       calling the callback function for each regular file'''\n",
    "    global dict\n",
    "    for f in os.listdir(TopMostPath):\n",
    "        pathname = os.path.join(TopMostPath, f)\n",
    "        mode = os.stat(pathname)[ST_MODE]\n",
    "        if S_ISDIR(mode):\n",
    "            # It's a directory, recurse into it\n",
    "            walktree(pathname, callback)\n",
    "        elif S_ISREG(mode):\n",
    "            # It's a file, call the callback function\n",
    "            callback(pathname)\n",
    "        else:\n",
    "            # Unknown file type, print a message\n",
    "            print 'Skipping %s' % pathname\n",
    "\n",
    "\n",
    "def sentiment(file):\n",
    "    \n",
    "    global i \n",
    "    global interval_segments, sentiment_segments\n",
    "    #the srt files we need to train our model\n",
    "    if '.srt' in file:\n",
    "        interval_segments, sentiment_segments = get_sentiment(file)\n",
    "        #fig, ax = plt.subplots()\n",
    "        #plt.plot(interval_segments,sentiment_segments)\n",
    "        #plt.title(file, fontsize=32)\n",
    "        #plt.ylim((-1, 1))\n",
    "        #plt.ylabel(\"Sentiment Polarity\")\n",
    "        #plt.xlabel(\"Running Time\")\n",
    "        #plt.text(.5, 1.03, \"Average Sentiment - \" + str(round(average(y), 4)), color=\"green\")\n",
    "        \n",
    "        #DICTIONARY PATH,INTERVALS,SENTIMENTS\n",
    "        dict[i]={}\n",
    "        dict[i]['Id']=i\n",
    "        dict[i]['path']=file\n",
    "        dict[i]['intervals']=interval_segments\n",
    "        dict[i]['sentiments']=sentiment_segments\n",
    "        i=i+1\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "--------- exclude tag --------5\n",
      "--------- exclude tag --------8\n",
      "--------- exclude tag --------12\n",
      "--------- exclude tag --------16\n",
      "--------- exclude tag --------17\n",
      "--------- exclude tag --------32\n",
      "--------- exclude tag --------35\n",
      "--------- exclude tag --------54\n",
      "--------- exclude tag --------59\n",
      "--------- exclude tag --------61\n",
      "--------- exclude tag --------62\n",
      "--------- exclude tag --------66\n",
      "--------- exclude tag --------81\n",
      "--------- exclude tag --------86\n",
      "--------- exclude tag --------89\n",
      "--------- exclude tag --------91\n",
      "84\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "path = '/home/mscuser/multimodal/multimodal_audio'\n",
    "walktree(path, sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': '/home/mscuser/multimodal/multimodal_audio/Travis Scott Shows Off His Broadway Musical Abilities-QV9nArq7phA.en.srt', 'intervals': [SubRipTime(0, 0, 1, 634), SubRipTime(0, 0, 2, 868), SubRipTime(0, 0, 4, 604), SubRipTime(0, 0, 5, 971), SubRipTime(0, 0, 7, 707), SubRipTime(0, 0, 11, 910), SubRipTime(0, 0, 13, 746), SubRipTime(0, 0, 19, 119), SubRipTime(0, 0, 20, 419), SubRipTime(0, 0, 22, 21), SubRipTime(0, 0, 26, 92), SubRipTime(0, 0, 27, 794), SubRipTime(0, 0, 30, 563), SubRipTime(0, 0, 37, 403), SubRipTime(0, 0, 38, 604), SubRipTime(0, 0, 41, 341), SubRipTime(0, 0, 44, 144), SubRipTime(0, 0, 46, 812), SubRipTime(0, 0, 48, 914), SubRipTime(0, 0, 51, 150), SubRipTime(0, 0, 52, 618), SubRipTime(0, 0, 54, 454), SubRipTime(0, 0, 58, 357), SubRipTime(0, 1, 1, 728), SubRipTime(0, 1, 3, 729), SubRipTime(0, 1, 5, 431), SubRipTime(0, 1, 8, 801), SubRipTime(0, 1, 13, 173), SubRipTime(0, 1, 15, 141), SubRipTime(0, 1, 19, 679), SubRipTime(0, 1, 22, 515), SubRipTime(0, 1, 24, 317), SubRipTime(0, 1, 25, 685), SubRipTime(0, 1, 27, 986), SubRipTime(0, 1, 30, 856), SubRipTime(0, 1, 32, 558), SubRipTime(0, 1, 33, 592), SubRipTime(0, 1, 37, 163), SubRipTime(0, 1, 38, 731), SubRipTime(0, 1, 40, 66), SubRipTime(0, 1, 42, 835), SubRipTime(0, 1, 44, 270), SubRipTime(0, 1, 47, 573), SubRipTime(0, 1, 49, 575), SubRipTime(0, 1, 51, 377), SubRipTime(0, 1, 54, 580), SubRipTime(0, 1, 56, 749), SubRipTime(0, 2, 0, 220), SubRipTime(0, 2, 1, 520), SubRipTime(0, 2, 4, 189), SubRipTime(0, 2, 6, 625), SubRipTime(0, 2, 11, 297), SubRipTime(0, 2, 15, 601), SubRipTime(0, 2, 17, 636), SubRipTime(0, 2, 18, 771), SubRipTime(0, 2, 22, 508), SubRipTime(0, 2, 25, 144), SubRipTime(0, 2, 26, 412), SubRipTime(0, 2, 28, 114), SubRipTime(0, 2, 29, 716), SubRipTime(0, 2, 30, 816), SubRipTime(0, 2, 33, 18), SubRipTime(0, 2, 35, 688), SubRipTime(0, 2, 37, 90), SubRipTime(0, 2, 38, 724), SubRipTime(0, 2, 41, 427), SubRipTime(0, 2, 42, 528), SubRipTime(0, 2, 44, 664), SubRipTime(0, 2, 46, 932), SubRipTime(0, 2, 50, 369), SubRipTime(0, 2, 51, 437), SubRipTime(0, 2, 53, 105), SubRipTime(0, 2, 54, 140), SubRipTime(0, 2, 59, 979), SubRipTime(0, 3, 2, 648), SubRipTime(0, 3, 7, 386), SubRipTime(0, 3, 10, 223), SubRipTime(0, 3, 12, 992), SubRipTime(0, 3, 15, 661), SubRipTime(0, 3, 16, 929), SubRipTime(0, 3, 18, 731), SubRipTime(0, 3, 21, 301), SubRipTime(0, 3, 22, 634), SubRipTime(0, 3, 23, 903)], 'sentiments': [0.0, 0.0, 0.25, 0.4, -0.3125, 0.2, 1.0, 0.0, 0.0, 0.0, 0.0, 0.024999999999999994, 0.0, 0.0, 0.0, 0.0, -0.25, 0.0, 0.0, -0.3125, 0.0, 0.0, 0.5, 0.2, 0.0, 0.0, 0.0, 0.0, -0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.5, 0.0, 0.5, 0.0, 0.0, 0.4928571428571428, 0.0, -0.1875, 0.0, 0.0, 0.16, 0.0, 0.2, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.2, 0.39999999999999997, 0.0, 0.0, 0.0, 0.1, 0.25, 0.0, -0.125, 0.0, -0.05, 0.2, 0.0, -0.25, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3125, 0.0, 0.0, 0.0, 0.0, 0.0], 'Id': 0}\n"
     ]
    }
   ],
   "source": [
    "print dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a csv for all videos that contains:videoid, path, list of interval segments, list of sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(dict)):\n",
    "    with open('captions_polarity.csv', 'wb') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for key, value in dict.items():\n",
    "            writer.writerow([key, value])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#print len(sentiment_segments)\n",
    "#print sentiment_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print interval_segments[0]\n",
    "#print interval_segments[1]\n",
    "#print interval_segments[99]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Polarity>0-->Positive, POlarity<0-->Negative, Polarity=0-->Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a csv for each video that contains: segment interval, pollarity for all  segments of the video.  The name of the file is polarity follwed by videoId. eg:polarity0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dicts in range(len(dict)):\n",
    "    new_csv='polarity'+str(dict[dicts]['Id']) +'.csv'\n",
    "    with open(new_csv, 'wb') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for j in range(len(dict[dicts]['intervals'])):\n",
    "            context=(dict[dicts]['intervals'][j],dict[dicts]['sentiments'][j])\n",
    "            writer.writerows([context])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "          "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#    Find the mean of positives, negatives per movie and add it at them at teh existing dictionary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for i in range(len(dict)):\n",
    "    #for earch movie\n",
    "    positives=[]\n",
    "    negatives=[]\n",
    "    for j in range(len(dict[i][2])):\n",
    "        if float(dict[i][2][j])>float(0):\n",
    "            positives.append(dict[i][2][j])\n",
    "        elif float(dict[i][2][j])<0:\n",
    "            negatives.append(dict[i][2][j])\n",
    "   \n",
    "    \n",
    "    dict[i]=dict[i] + (np.mean(positives),)\n",
    "    dict[i]=dict[i] + (np.mean(negatives),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "favorite_color = { \"lion\": \"yellow\", \"kitty\": \"red\" }\n",
    "\n",
    "for dicts in range(len(dict)):\n",
    "    context=[]\n",
    "    pickle_name='polarity'+str(dict[dicts]['Id']) +'.p'\n",
    "    for j in range(len(dict[dicts]['intervals'])):\n",
    "        context.append((dict[dicts]['intervals'][j],dict[dicts]['sentiments'][j]))\n",
    "    pickle.dump( context, open( pickle_name, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
